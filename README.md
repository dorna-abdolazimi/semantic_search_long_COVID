# Semantic Search on Long COVID (PASC) Publications

In this project, we are interested in dense information retrieval (IR) in the database of scientific papers related to long COVID (PASC) that are available on PubMed. There are a variey of [pretrained sentence transformers](https://huggingface.co/sentence-transformers) for IR/semantic search on SentenceTransformers ðŸ¤—. These models were trained on datasets such as [PAQ](https://github.com/facebookresearch/PAQ), [MSMARCO](https://microsoft.github.io/msmarco/), [GoogleAQ](https://github.com/allenai/gooaq) and, while their general performance is good, they can perform rather poorly on domains that are very different from the domain that they were trained on. The main challenge in **domain adaptation** for IR is the lack of labeled training data for the specific domian of interest. Adaptive pre-training methods such as [TSDAE](https://arxiv.org/pdf/2104.06979) and [generative pseudo-labeling (GPL)](https://arxiv.org/pdf/2112.07577) are the main two techniques that have been proposed for this scenario. In this project, we use **GPL** to finetune [msmarco-distilbert-dot-v5](https://huggingface.co/sentence-transformers/msmarco-distilbert-dot-v5) for IR in our domain.

### Motivation for information retrieval on publications related to long COVID
 Long COVID, also known as Post-Acute Sequelae of SARS-CoV-2 infection (PASC), is defined by WHO as the continuation or development of new symptoms 3 months after the initial SARS-CoV-2 infection, with these symptoms lasting for at least 2 months with no other explanation. PASC is a multi-organ disease for which over 200 different symptoms have been reported. While it is estimated that [about 7% of the population ](https://ceal.nih.gov/sites/default/files/2023-02/CEAL-WhatYouNeedtoKnowLongCOVID-English.pdf) suffers from PASC, this complex disease and possible treatments are not well-understood. As research on this prevalent and multi-organ condition is rapidly evolving, it is important for general practitioners, patients, and other indivisuals concerned or interested in this disease to be able to answer relevant questions based on the evolving research using an information retrieval system that performs well on related publications.

 ### Motivation for the use of GPL
 Adaptive pre-training methods first pre-train on the unlabeled target corpus using methods such MLM or TSDAE and then finetune on an existing labeled dateset. A major disadvantage of these methods is that these methods are very expensive computationally. To get a desirable results, one has to use a very varge labeled training dataset (of the order of tens of millions of training pairs/triplets). On the other hand, GPL could be used to finetune a pretrained sentence transformer/bi-encoder model using a generated training set of much smaller size (e.g. less than half a million triplets). While the performance of GPL is often not as high as TSDAE, it often achieves significant improvement over the zero-shot model (for some comparisons of different methods see Table 1 in the [GPL paper](https://arxiv.org/pdf/2112.07577). Given limited compuation resources for this personal project, I chose GPL over adaptive pre-training methods.

 ### Result
 I compared the performance of the finetuned model with the original [msmarco-distilbert-dot-v5](https://huggingface.co/sentence-transformers/) model on a set of 30 questions that include both general questions such as the prevalence and the risk factors and advanced scientific questions such as the pathophysiology of PASC. While the origial model had a high performance on general queries, its performance was poor on advanced topics. **Using the finetuned model, the mAP@10 (Mean Average Precision) increased from 0.646 to 0.704 (8% relative increase)**, mainly due to significantly better performance on queries related to more advanced scientific topics.

 ### Datasets and the finetuned model
 
All datasets used in this notebook were created from scratch by the code available in the notebook. These datasets and the finetuned model are available [here](https://drive.google.com/drive/folders/1uF6z15JKNhrC-ICKtOrCdRxCkFjTL17R?usp=drive_link).  
